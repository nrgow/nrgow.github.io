<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.10.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-07-10T12:03:55+02:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Token-mediated</title><subtitle>Work in progress üöß</subtitle><entry><title type="html">Spoken Language Entity Linking: Embedding Model</title><link href="http://localhost:4000/2025/07/09/spoken-language-embedding-model.html" rel="alternate" type="text/html" title="Spoken Language Entity Linking: Embedding Model" /><published>2025-07-09T13:01:09+02:00</published><updated>2025-07-09T13:01:09+02:00</updated><id>http://localhost:4000/2025/07/09/spoken-language-embedding-model</id><content type="html" xml:base="http://localhost:4000/2025/07/09/spoken-language-embedding-model.html"><![CDATA[<p>With out synthetic data prepared we can now train an embedding model and attempt an initial evaluation of the whole system. The embedding model will have a byt5 backbone. Why byt5? Shouldn‚Äôt the base model by a hyperparameter?  I will state without proof that this task requires a tokenizer-free approach. Standard LLMs with learned subword tokenizers are notoriously bad at understanding the internal structure of their subwords, and the phonetic similarity task requires just such understanding. There are some other tokenizer-free models, such as canine which could be interesting as well. But I‚Äôll start with <code class="language-plaintext highlighter-rouge">byt5-small</code>, with mean pooling and an embedding dimension of 256. The data is a combination of transcriptions from parakeet and whisper-turbo.</p>

<p>We only have positive examples of known word-transcription pairs, so we‚Äôll need to do negative mining to get negative examples. For want of a better option right now, let‚Äôs generate them using the <code class="language-plaintext highlighter-rouge">sentence-transformers/all-MiniLM-L6-v2</code> model.</p>

<h1 id="baseline-model-character-ngrams">Baseline model: character ngrams</h1>

<p>As mentioned in the introduction post, when going down the road of neural networks, GPUs and training models, we need to first pause and determine if all this is really necessary. Perhaps there is a simpler way.</p>

<p>We can try subword ngrams.</p>

<p><strong>TODO example here</strong></p>

<p>Things get a bit tricky here: following experience of elasticsearch, I start to look at Qdrant‚Äôs tokenizer implementations. There exists no character ngram tokenizer. I fire up gemini code and ask it to implement that for me. After a few minutes of mostly auto-accept and a little hinting, the feature is apparently ready. Then I am not sure if this is even right path to go. I know I need to sparse vector type, but I am not sure of the relationship between Qdrant full-text search and sparse vectors. Ultimately it seems little easier if I just create the sparse vectors from character ngrams myself, then I don‚Äôt need a custom tokenizer in the database.</p>

<p>One minor annoyance about handling ngram creation myself is that I would need to maintain a mapping from ngrams to vector index. But of course we can use the hashing trick to simply hash the ngrams and and interpreting the hash as a uint32 we have the indices. It turns out that this is exactly what Qdrant does in their own <em>fast-embed</em> library. Hash collisions may occur but they will add little to the overall error rate.</p>

<h1 id="baseline-model-evaluation">Baseline model evaluation</h1>

<p>Let‚Äôs take recall@10 as the evaluation metric - is the right answer in the top-10 results. The task is relatively hard so I don‚Äôt expect top-1 accuracy to be very good. But we can look at that too. Recall@10 also gives us an upper bound on the accuracy of a reranking approach that reranked the top 10 results.</p>

<p>Recall@10 is around 40-42%. Not great, but maybe better than expected.</p>

<p><strong>TODO show nearest neighbors examples from baseline model</strong></p>

<p>One of issues is that the the nearest neighbors are often totally implausible. We cannot expect too much as a bag-of-ngrams loses all ordering information.</p>

<h1 id="embedding-model">Embedding model</h1>

<p><strong>TODO show tensorboard metrics graph for training the first model</strong></p>

<h2 id="first-attempt">First attempt</h2>

<h2 id="the-solution-harder-negative-mining">The solution: hard(er) negative mining</h2>

<p>At this point, one may ask what went wrong, or abandon the whole enterprise entirely. Maybe something with training, like overfitting, or maybe the synthetic data is just too low quality. For me the most likely suspect is indeed the training data, but specifically, how the negative samples were generated. We used ‚Äúsentence-transformers/all-MiniLM-L6-v2‚Äù to generate negative examples, but as per the foregoing, it has no notion of phonetics, so the negative examples are unlikely to be very useful.</p>

<p>We need <em>hard negatives</em>. Maybe the baseline ngram model could generate them? This could work, but actually I decide to just use embedding model itself to generate hard negatives. This v0 embedding model doesn‚Äôt perform great but it will generate better negatives than MiniLM. <strong>TODO show examples proving this statement</strong></p>

<p>After retraining, the evaluation looks much better. We get recall@10 of 78%. If we are really interested with a single result, recall@1 is 51%, and a reranker could get an upper-bound of 86% if it had access to the top-40 examples.</p>

<p><strong>TODO show nearest neighbors examples from improved model</strong></p>

<table>
  <thead>
    <tr>
      <th>at_k</th>
      <th>recall@1</th>
      <th>¬†</th>
      <th>recall@10</th>
      <th>¬†</th>
      <th>recall@40</th>
      <th>¬†</th>
    </tr>
    <tr>
      <th>asr</th>
      <th>parakeet</th>
      <th>whisper-turbo</th>
      <th>parakeet</th>
      <th>whisper-turbo</th>
      <th>parakeet</th>
      <th>whisper-turbo</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>sparse</td>
      <td>0.21</td>
      <td>0.22</td>
      <td>0.40</td>
      <td>0.42</td>
      <td>0.53</td>
      <td>0.53</td>
    </tr>
    <tr>
      <td>dense-bad</td>
      <td>0.29</td>
      <td>0.31</td>
      <td>0.51</td>
      <td>0.52</td>
      <td>0.63</td>
      <td>0.64</td>
    </tr>
    <tr>
      <td>dense-better</td>
      <td>0.51</td>
      <td>0.52</td>
      <td>0.78</td>
      <td>0.78</td>
      <td>0.86</td>
      <td>0.86</td>
    </tr>
  </tbody>
</table>

<p><em>The scores for different models and metrics on the validation set</em></p>

<h1 id="outlook">Outlook</h1>
<p>We now perform significantly better than the baseline, which leaves us at a crossroads. Where to go next to improve the system?</p>

<h2 id="reranking">Reranking</h2>
<p>We can just as easily train a reranker base on a biencoder. Reranking from 40 candidates could give us up to 35 p.p. improvement in top-1 accuracy. Sounds tempting.</p>

<h2 id="ensembling-over-sparsedense-vector-models-hybrid-search">Ensembling over sparse/dense vector models (hybrid search)</h2>
<p>The dense model does not strictly dominate the sparse model: there are cases where the correct result is in the top-10 sparse results but not in the dense, and vice-versa. So there‚Äôs possibly something to be gained from a combination. As a rough upper-bound, for e.g. the parakeet ASR model, we could check if the correct result in either the sparse top-10 <em>or</em> the dense top-10. That gives us 80%.</p>

<h2 id="end-to-end-ensembling-over-asr-models">End to end ensembling over ASR models</h2>
<p>The idea is this: if two ASR models make somewhat uncorrelated errors, then, for a given user query, we could run both models, and generate embeddings for both transcripts. The two vectors could then be averaged. These averaged vector will have lower variance than those coming from a single model. We can quickly compute something like an upper-bound for how much better our recall@10 would be, by asking per-entity, is the right answer in the top-10 result for either parakeet or whisper-turbo. This would give 86%. So there is potentially more to gain from ensembling over ASR transcriptions, rather than over vector search methods, and indeed perhaps delivers roughly the same gain that a reranker could. These results are not so precise because we are would now be considering up to 20 items rather than 10, and secondly, the results from the averaged embedding are not always coming from the union of the results of the two input vectors.</p>

<h2 id="retrieval-augmented-audio-text-hypothesis-rescoring">Retrieval-augmented audio-text hypothesis rescoring</h2>
<p>Now that we have a passable retrieval model, we could try using a multimodal llm to reanalyse the audio given 10 likely transcription candidates: this sounds so crazy it just might work. The idea being, either zero shot, or via finetuning, to use the audio LLM as a reranker that not only has access to the candidate texts, but also the audio. A multimodal LLM gives us this potential and more.</p>

<h1 id="speed">Speed</h1>
<p>I can load 3k sparse vectors per second into Qdrant. That‚Äôs slower than into elasticsearch for some reason. And queries reach 3-200 per second. I can load 1.3k dense vectors per second, and that includes the time to embed on my local GPU enabled machine, and I didn‚Äôt bother running embedding in parallel, which one should. I made some use of the <a href="https://qdrant.tech/articles/indexing-optimization/">batch insert performance guide</a> but it can probably load faster. Querying reaches 50/second after the HNSW index finished construction.</p>

<h1 id="whats-next">What‚Äôs next?</h1>
<p>In the next post I‚Äôll reveal how this system performs on the human control data.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[With out synthetic data prepared we can now train an embedding model and attempt an initial evaluation of the whole system. The embedding model will have a byt5 backbone. Why byt5? Shouldn‚Äôt the base model by a hyperparameter? I will state without proof that this task requires a tokenizer-free approach. Standard LLMs with learned subword tokenizers are notoriously bad at understanding the internal structure of their subwords, and the phonetic similarity task requires just such understanding. There are some other tokenizer-free models, such as canine which could be interesting as well. But I‚Äôll start with byt5-small, with mean pooling and an embedding dimension of 256. The data is a combination of transcriptions from parakeet and whisper-turbo.]]></summary></entry><entry><title type="html">Spoken Language Entity Linking: Synthetic Data</title><link href="http://localhost:4000/2025/07/07/spoken-language-synthetic-data.html" rel="alternate" type="text/html" title="Spoken Language Entity Linking: Synthetic Data" /><published>2025-07-07T13:01:09+02:00</published><updated>2025-07-07T13:01:09+02:00</updated><id>http://localhost:4000/2025/07/07/spoken-language-synthetic-data</id><content type="html" xml:base="http://localhost:4000/2025/07/07/spoken-language-synthetic-data.html"><![CDATA[<p>Let‚Äôs take a look at the synthetic data for the spoken language entity linking system. We‚Äôll need a list of target entities, audio of users requesting those entities, and transcriptions of that audio.</p>

<p>We can download all open street map data for our countries of interest via <a href="geofabrik.de">Geofabrik</a>. The data is a rather large protobuf file which can be converted by <code class="language-plaintext highlighter-rouge">osmconvert</code> into larger xml file. Attempts to efficiently parse this thing and extract street names via with a streaming xml parser stumped Gemini code - presumably it only has access to the same 15 year old blog posts as I could find. In the end, one can just grep for the relevant parts of the xml structure, reducing the file size considerably, then do the streaming xml parsing to precisely extract the information.</p>

<p>We end up with about 500k unique street names, which we can split into train, validation and test sets.</p>

<h1 id="text-to-speech">Text to speech</h1>

<p>The diagram in the <a href="https://nrgow.github.io/2025/07/03/spoken-language-entity-linking.html">first post</a> described the flow. <a href="https://huggingface.co/hexgrad/Kokoro-82M">Kokoro</a> can generate from phonemes, so we provide the phonemes for German street names from a German phonemizer, Dutch streets from a Dutch phonemizer, and so on - all espeak-ng phoneme models.</p>

<audio controls="">
  <source src="/assets/audio/plommonvagen_sv.wav" type="audio/mpeg" />
  Your browser does not support the audio element.
</audio>

<p><em>‚ÄúNavigate to Plommonv√§gen‚Äù</em></p>

<audio controls="">
  <source src="/assets/audio/wustenwetzdorf_de.wav" type="audio/mpeg" />
  Your browser does not support the audio element.
</audio>

<p><em>‚ÄúNavigate to W√ºstenwetzdorf‚Äù</em></p>

<audio controls="">
  <source src="/assets/audio/meerswal_nl.wav" type="audio/mpeg" />
  Your browser does not support the audio element.
</audio>

<p><em>‚ÄúNavigate to Meerswal‚Äù</em></p>

<p>There‚Äôs plenty of other things that could be done, like audio augmentations, considering alternative TTS providers. But there is enough to work with here for the moment. I sample 50k street names to generate audio for.</p>

<h1 id="speech-to-text">Speech to text</h1>

<p>For transcribing our synthetic data, let‚Äôs start with <em>whisper-turbo</em> - this is a more compact version of a multilingual ASR model, which in my experience can sometimes do surprisingly well with code-switched queries.</p>

<p>As an alternative, we can look at a more recent model: Parakeet (<em>nvidia/parakeet-tdt-0.6b-v2</em>) is and performant model from nvidia. It has the benefit of being ~20x faster than whisper-turbo. It currently second on the <a href="https://huggingface.co/spaces/hf-audio/open_asr_leaderboard">open ASR leaderboard</a>. It‚Äôs English-only. Despite this project being about code-switching, one of the secret premises of this project is that we might even not need to know much about other languages, at least to do this somewhat artificial entity linking task. That‚Äôs if all goes well.</p>

<p>The character error rate over the entities is pretty similar for parakeet vs. whisper-turbo (43%).</p>

<h2 id="multimodal-llms">Multimodal LLMs</h2>

<p>Some new multimodal LLMs have been released recently, most notably phi-4-multimodal and gemma-3n. These accept speech audio or text (images as well) as input. Why might such a model be interesting for a simple transcription task? Incorporation of a text-based prompt potentially allows for zero-shot or few-shot customization of the ASR component without requiring finetuning. Let‚Äôs see if that works with Gemma 3n <em>google/gemma-3n-E2B-it</em>.</p>

<h3 id="prompt-help-1">Prompt help 1</h3>

<p>All queries have the form ‚Äúnavigate to <street>", which I use to then extract the street name via regex. I already have to Gemma give a little help via the system prompt.</street></p>

<blockquote>
  <p>‚ÄúYou are a helpful assistant who can transcribe audio. Users typically issue commands <em>in the imperative voice</em>.‚Äù</p>
</blockquote>

<p>The results are pretty bad. Character error rate is 57%. The transcriptions are not always phonetically plausible, rather making phonetically poorly-grounded semantic jumps to other entities, but we‚Äôre not here to judge ASR models. We‚Äôre trying to train a model that can correct for ASR errors, so the transcription may be valuable for that.</p>

<h3 id="prompt-help-2">Prompt help 2</h3>

<p>As an experiment, let‚Äôs give Gemma some more help with an enhanced task prompt:</p>

<blockquote>
  <p>Transcribe this audio file which has this structure: ‚ÄòNavigate to &lt;german/dutch/swedish street name&gt;‚Äô</p>
</blockquote>

<p>Things start to look better. Character error rate decreases to 46%: worse than parakeet or whisper but closing the gap. In less than one percent of cases, it takes the prompt a little bit too literally, and outputs ‚Äú<em>navigate to &lt;german/dutch/swedish street name&gt;</em>‚Äù.</p>

<p>This example may be a little contrived, as one might rarely be able to give such a big hint, but it nicely demonstrates the flexibility and responsiveness of a multimodal model to enhanced context. In a real system, such context engineering could go in the direction of personalization and memory, or guidelines for particular terminology.</p>

<h2 id="interim-character-error-rate-evaluation">Interim character error-rate evaluation</h2>

<p>We can measure the ASR on our synthetic data, taking the character error rate of just the entity name. This evaluation is just for demonstration - maybe it is roughly correlated with end to end task performance.</p>

<table>
  <thead>
    <tr>
      <th>¬†</th>
      <th>parakeet</th>
      <th>whisper-turbo</th>
      <th>gemma_simpleprompt</th>
      <th>gemma_hintprompt</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>cer</td>
      <td>0.43</td>
      <td>0.43</td>
      <td>0.57</td>
      <td>0.46</td>
    </tr>
  </tbody>
</table>

<h2 id="transcription-examples">Transcription examples</h2>

<table>
  <thead>
    <tr>
      <th>entity</th>
      <th>parakeet</th>
      <th>whisper-turbo</th>
      <th>gemma_simpleprompt</th>
      <th>gemma_hintprompt</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>K√∂lv√§gen</td>
      <td>Schellwegan</td>
      <td>Shellvegan</td>
      <td>Shelvegen</td>
      <td>Shelvegan</td>
    </tr>
    <tr>
      <td>Pfarrer-Held-Stra√üe</td>
      <td>Pfaterhilshthasa</td>
      <td>Pfaderhildst√§dt Hasse</td>
      <td>Powerchords to Hassa</td>
      <td>Pfahlstr</td>
    </tr>
    <tr>
      <td>Park de Wervelaan</td>
      <td>Parc de Vervelin</td>
      <td>Parc de Vervalan</td>
      <td>Park de Verelan</td>
      <td>Park de Verellen</td>
    </tr>
    <tr>
      <td>Untere Pfeiferm√ºhle</td>
      <td>Unterup Phi formula</td>
      <td>Unterupfi Formula</td>
      <td>Unter a five formula</td>
      <td>Unter der Pfeiferstra√üe</td>
    </tr>
    <tr>
      <td>Posses v√§g</td>
      <td>Pausus Veg</td>
      <td>Posis Veg</td>
      <td>Passus Veg</td>
      <td>Passusweg</td>
    </tr>
    <tr>
      <td>Am Springgarten</td>
      <td>Mspttingaten</td>
      <td>Amschptingaten</td>
      <td>Amsterdam</td>
      <td>amshiptingaten</td>
    </tr>
    <tr>
      <td>Bro√§ngsv√§gen</td>
      <td>Brunswagen</td>
      <td>Bruinswagen</td>
      <td>Braunschweig</td>
      <td>Braunschweig</td>
    </tr>
    <tr>
      <td>Alander Weg</td>
      <td>Aulender Vake</td>
      <td>All under VAKE</td>
      <td>all under wake</td>
      <td>Allenderwiek</td>
    </tr>
    <tr>
      <td>Im Alengarten</td>
      <td>him all in Gatson</td>
      <td>Im Allenghattan</td>
      <td>M All in Gaten</td>
      <td>am Allingaten</td>
    </tr>
    <tr>
      <td>Nordbrink</td>
      <td>NodPink</td>
      <td>Nodbdink</td>
      <td>Notepad</td>
      <td>Nordpink</td>
    </tr>
    <tr>
      <td>Siegelhof</td>
      <td>Ziegelhoof</td>
      <td>Ziegelhof</td>
      <td>Ziegelhof</td>
      <td>Zeughof</td>
    </tr>
    <tr>
      <td>Im Eisenfeld</td>
      <td>him Eisenfelt</td>
      <td>M. Eisenfeld</td>
      <td>Imfeld</td>
      <td>Immenfeld</td>
    </tr>
  </tbody>
</table>

<h1 id="human-control-audio">Human control audio</h1>

<p>It‚Äôs pretty fast to just say things, so collecting a bit of real data for validating things at the end is very doable. <em>If only there was a nice tool to handle keep track of all the wav files you need to generate</em>. Gemini code can easily cook one up in 20 minutes (that‚Äôs including a game of Minesweeper intermediate I played in between - really it was closer to 2 minutes).</p>

<p>Wielding this new tool, I can quickly generate 50 test set queries, and 50 validation set queries, which I‚Äôll evaluate on the end to end system later.</p>

<p><img src="/assets/images/data-collection-tool.png" alt="Let's generate some human data too" /></p>

<p><em>Let‚Äôs generate some human data too</em></p>

<p>With some transcriptions in hand, we can now start to train the embedding model.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Let‚Äôs take a look at the synthetic data for the spoken language entity linking system. We‚Äôll need a list of target entities, audio of users requesting those entities, and transcriptions of that audio.]]></summary></entry><entry><title type="html">Code-Switched Spoken Language Entity Linking: Intro</title><link href="http://localhost:4000/2025/07/03/spoken-language-entity-linking.html" rel="alternate" type="text/html" title="Code-Switched Spoken Language Entity Linking: Intro" /><published>2025-07-03T23:40:42+02:00</published><updated>2025-07-03T23:40:42+02:00</updated><id>http://localhost:4000/2025/07/03/spoken-language-entity-linking</id><content type="html" xml:base="http://localhost:4000/2025/07/03/spoken-language-entity-linking.html"><![CDATA[<p>Let‚Äôs say you‚Äôre on holiday with your voice assistant, and you want to get to <em>Bolzeschachtstra√üe</em>. You fire off a wake-up word, recording starts, you say: ‚Äú<em>Navigate to Bolzeschachtstra√üe</em>‚Äù. Your voice assistant hears ‚Äú<em>Baltzsakstasa</em>‚Äù. Another day, another place, this time <em>Zum Hallerbach</em>. Your voice assistant hears ‚Äú<em>Tsamhalaba</em>‚Äù. No results.</p>

<p><img src="/assets/images/typical_scenario.png" alt="Can we fix this?" width="40%" /></p>

<p><em>Can we fix this?</em></p>

<p>What is to blame? Clearly, speech recognition has a problem. It‚Äôs great if you can fix it. On the other hand, a sufficiently smart voice assistant really <em>should</em> be able to guess, <em>even from a severely distorted transcription</em>, what is really intended, as long as the distortion is not totally wild.</p>

<p>The following blogposts will demonstrate a simple system that can take such garbled transcriptions from speech recognition and offer more accurate alternatives.</p>

<p>‚Äú<em>Code-switched</em>‚Äù means that the user will be speaking English, but will be requesting <em>non-English</em> entities. To keep things compact, I will define the universe of entities as being: German, Dutch and Swedish street names. A large enough universe to make the task challenging. I may add more as time goes on. ‚Äú<em>Spoken language</em>‚Äù means that the input to the system is speech audio, to be transcribed by an ASR module.</p>

<p>The core of this system will be a text-based phonetic embedding model. The similarity function the embedding model should try to capture is this: ‚Äústring <em>x</em> should be close to string <em>y</em> if <em>x</em> is a likely transcription from an ASR system of someone speaking <em>y</em>‚Äù. So, phonetics is the main invariance that should be captured, but text normalization is another, as well as the output quirks of specific ASR models. The transformer backbone of the model will by a <strong>byt5</strong> encoder. I‚Äôll also consider a (not so?) silly baseline model based on <em>character ngrams</em>.</p>

<p>When people think of phonetic similarity, they may think of things like levenshtein distance. Unfortunately this won‚Äôt scale into a retrieval system, so we‚Äôll have to use a sparse or dense search index. The character ngram approach might be considered an approximation of edit distance. Out of the box, it already seems pretty weak because it doesn‚Äôt respect any of the invariances we are interested in. Maybe we can overcome this. In any case, there is, to my knowledge, no plug-and-play phonetic embedding model out there, so I‚Äôll have to build this.</p>

<p>I‚Äôll take a look at a few different ASR models, phonetic embedding training strategies, and hybrid search &amp; reranking approaches. I‚Äôll use <em>Qdrant</em> as the vector search engine. For ASR, I will first consider <strong>openai/whisper-large-v3-turbo</strong> and <strong>nvidia/parakeet-tdt-0.6b-v2</strong>. But maybe some more recent multimodal LLMs which can accept audio input, like <strong>google/gemma-3n-E2B-it</strong> and <strong>microsoft/phi-4-instruct</strong>.</p>

<p><img src="/assets/images/embedding-training-pipeline.drawio.png" alt="Here's how it will all look" /></p>

<p><em>Here‚Äôs how the embedding model will be trained</em></p>

<p>The training data for the embedding model will come from <strong>hexgrad/Kokoro-82M</strong> text-to-speech. It has the advantage that you can synthesize from phonemes. The idea is to phonemize the street name part of the TTS queries using a locale-appropriate phonemizer, to approximate an English speaker who can roughly pronounce the non-English street name. If one of the good commercial providers can do code switching better than this, I might spend O(‚Ç¨10) on that too.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Let‚Äôs say you‚Äôre on holiday with your voice assistant, and you want to get to Bolzeschachtstra√üe. You fire off a wake-up word, recording starts, you say: ‚ÄúNavigate to Bolzeschachtstra√üe‚Äù. Your voice assistant hears ‚ÄúBaltzsakstasa‚Äù. Another day, another place, this time Zum Hallerbach. Your voice assistant hears ‚ÄúTsamhalaba‚Äù. No results.]]></summary></entry></feed>