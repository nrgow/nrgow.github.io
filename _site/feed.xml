<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.10.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-07-07T14:26:13+02:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Token-mediated</title><subtitle>Work in progress üöß</subtitle><entry><title type="html">Spoken Language Entity Linking: Synthetic Data</title><link href="http://localhost:4000/2025/07/07/spoken-language-synthetic-data.html" rel="alternate" type="text/html" title="Spoken Language Entity Linking: Synthetic Data" /><published>2025-07-07T13:01:09+02:00</published><updated>2025-07-07T13:01:09+02:00</updated><id>http://localhost:4000/2025/07/07/spoken-language-synthetic-data</id><content type="html" xml:base="http://localhost:4000/2025/07/07/spoken-language-synthetic-data.html"><![CDATA[<p>Let‚Äôs take a look at the synthetic data for the spoken language entity linking system. We‚Äôll need a list of target entities, audio of users requesting those entities, and transcriptions of that audio.</p>

<p>We can download all open street map data for our countries of interest via <a href="geofabrik.de">Geofabrik</a>. The data is a rather large protobuf file which can be converted by osmconvert into larger xml file. Attempts to efficiently parse this thing and extract street names via with a streaming xml parser stumped Gemini code - presumably it only has access to the same 15 year old blog posts as I could find. In the end, one can just grep for the relevant parts of the xml structure, reducing the file size considerably, then do the streaming xml parsing to precisely extract the information.</p>

<p>We end up with about 500k unique street names, which we can split into train, validation and test sets.</p>

<h1 id="text-to-speech">Text to speech</h1>

<p>The diagram in the <a href="https://nrgow.github.io/2025/07/03/spoken-language-entity-linking.html">first post</a> described the flow. <a href="https://huggingface.co/hexgrad/Kokoro-82M">Kokoro</a> can generate from phonemes, so we provide the phonemes for German street names from a German phonemizer, Dutch streets from a Dutch phonemizer, and so on - all espeak-ng phoneme models.</p>

<audio controls="">
  <source src="/assets/audio/plommonvagen_sv.wav" type="audio/mpeg" />
  Your browser does not support the audio element.
</audio>

<p><em>‚ÄúNavigate to Plommonv√§gen‚Äù</em></p>

<audio controls="">
  <source src="/assets/audio/wustenwetzdorf_de.wav" type="audio/mpeg" />
  Your browser does not support the audio element.
</audio>

<p><em>‚ÄúNavigate to W√ºstenwetzdorf‚Äù</em></p>

<audio controls="">
  <source src="/assets/audio/meerswal_nl.wav" type="audio/mpeg" />
  Your browser does not support the audio element.
</audio>

<p><em>‚ÄúNavigate to Meerswal‚Äù</em></p>

<p>There‚Äôs plenty of other things that could be done, like audio augmentations, considering alternative TTS providers. But there is enough to work with here for the moment. I sample 50k street names to generate audio for.</p>

<h1 id="speech-to-text">Speech to text</h1>

<p>For transcribing our synthetic data, let‚Äôs start with <em>whisper-turbo</em> - this is a more compact version of a multilingual ASR model, which in my experience can sometimes do surprisingly well with code-switched queries.</p>

<p>As an alternative, we can look at a more recent model: Parakeet (<em>nvidia/parakeet-tdt-0.6b-v2</em>) is and performant model from nvidia. It has the benefit of being ~20x faster than whisper-turbo. It currently second on the <a href="https://huggingface.co/spaces/hf-audio/open_asr_leaderboard">open ASR leaderboard</a>. It‚Äôs English-only. Despite this project being about code-switching, one of the secret premises of this project is that we might even not need to know much about other languages, at least to do this somewhat artificial entity linking task. That‚Äôs if all goes well.</p>

<p>The character error rate over the entities is pretty similar for parakeet vs. whisper-turbo (43%).</p>

<h2 id="multimodal-llms">Multimodal LLMs</h2>

<p>Some new multimodal LLMs have been released recently, most notably phi-4-multimodal and gemma-3n. These accept speech audio or text (images as well) as input. Why might such a model be interesting for a simple transcription task? Incorporation of a text-based prompt potentially allows for zero-shot or few-shot customization of the ASR component without requiring finetuning. Let‚Äôs see if that works with Gemma 3n <em>google/gemma-3n-E2B-it</em>.</p>

<h3 id="prompt-help-1">Prompt help 1</h3>

<p>All queries have the form ‚Äúnavigate to <street>", which I use to then extract the street name via regex. I already have to Gemma give a little help via the system prompt.</street></p>

<blockquote>
  <p>‚ÄúYou are a helpful assistant who can transcribe audio. Users typically issue commands <em>in the imperative voice</em>.‚Äù</p>
</blockquote>

<p>The results are pretty bad. Character error rate is 57%. The transcriptions are not always phonetically plausible, rather making phonetically poorly-grounded semantic jumps to other entiti es, but we‚Äôre not here to judge ASR models. We‚Äôre trying to train a model that can correct for ASR errors, so the transcription may be valuable for that.</p>

<h3 id="prompt-help-2">Prompt help 2</h3>

<p>As an experiment, let‚Äôs give Gemma some more help with an enhanced task prompt:</p>

<blockquote>
  <p>Transcribe this audio file which has this structure: ‚ÄòNavigate to &lt;german/dutch/swedish street name&gt;‚Äô</p>
</blockquote>

<p>Things start to look better. Character error rate decreases to 46%: worse than parakeet or whisper but closing the gap. In less than one percent of cases, it takes the prompt a little bit too literally, and outputs ‚Äú<em>navigate to &lt;german/dutch/swedish street name&gt;</em>‚Äù.</p>

<p>This example may be a little contrived, as one might rarely be able to give such a big hint, but it nicely demonstrates the flexibility and responsiveness of a multimodal model to enhanced context. In a real system, such context engineering could go in the direction of personalization and memory, or guidelines for particular terminology.</p>

<h2 id="interim-character-error-rate-evaluation">Interim character error-rate evaluation</h2>

<p>We can measure the ASR on our synthetic data, taking the character error rate of just the entity name. This evaluation is just for demonstration - maybe it is roughly correlated with end to end task performance.</p>

<table>
  <thead>
    <tr>
      <th>¬†</th>
      <th>parakeet</th>
      <th>whisper-turbo</th>
      <th>gemma_simpleprompt</th>
      <th>gemma_hintprompt</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>cer</td>
      <td>0.43</td>
      <td>0.43</td>
      <td>0.57</td>
      <td>0.46</td>
    </tr>
  </tbody>
</table>

<h2 id="transcription-examples">Transcription examples</h2>

<table>
  <thead>
    <tr>
      <th>entity</th>
      <th>parakeet</th>
      <th>whisper-turbo</th>
      <th>gemma_simpleprompt</th>
      <th>gemma_hintprompt</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>K√∂lv√§gen</td>
      <td>Schellwegan</td>
      <td>Shellvegan</td>
      <td>Shelvegen</td>
      <td>Shelvegan</td>
    </tr>
    <tr>
      <td>Pfarrer-Held-Stra√üe</td>
      <td>Pfaterhilshthasa</td>
      <td>Pfaderhildst√§dt Hasse</td>
      <td>Powerchords to Hassa</td>
      <td>Pfahlstr</td>
    </tr>
    <tr>
      <td>Park de Wervelaan</td>
      <td>Parc de Vervelin</td>
      <td>Parc de Vervalan</td>
      <td>Park de Verelan</td>
      <td>Park de Verellen</td>
    </tr>
    <tr>
      <td>Untere Pfeiferm√ºhle</td>
      <td>Unterup Phi formula</td>
      <td>Unterupfi Formula</td>
      <td>Unter a five formula</td>
      <td>Unter der Pfeiferstra√üe</td>
    </tr>
    <tr>
      <td>Posses v√§g</td>
      <td>Pausus Veg</td>
      <td>Posis Veg</td>
      <td>Passus Veg</td>
      <td>Passusweg</td>
    </tr>
    <tr>
      <td>Am Springgarten</td>
      <td>Mspttingaten</td>
      <td>Amschptingaten</td>
      <td>Amsterdam</td>
      <td>amshiptingaten</td>
    </tr>
    <tr>
      <td>Bro√§ngsv√§gen</td>
      <td>Brunswagen</td>
      <td>Bruinswagen</td>
      <td>Braunschweig</td>
      <td>Braunschweig</td>
    </tr>
    <tr>
      <td>Alander Weg</td>
      <td>Aulender Vake</td>
      <td>All under VAKE</td>
      <td>all under wake</td>
      <td>Allenderwiek</td>
    </tr>
    <tr>
      <td>Im Alengarten</td>
      <td>him all in Gatson</td>
      <td>Im Allenghattan</td>
      <td>M All in Gaten</td>
      <td>am Allingaten</td>
    </tr>
    <tr>
      <td>Nordbrink</td>
      <td>NodPink</td>
      <td>Nodbdink</td>
      <td>Notepad</td>
      <td>Nordpink</td>
    </tr>
    <tr>
      <td>Siegelhof</td>
      <td>Ziegelhoof</td>
      <td>Ziegelhof</td>
      <td>Ziegelhof</td>
      <td>Zeughof</td>
    </tr>
    <tr>
      <td>Im Eisenfeld</td>
      <td>him Eisenfelt</td>
      <td>M. Eisenfeld</td>
      <td>Imfeld</td>
      <td>Immenfeld</td>
    </tr>
  </tbody>
</table>

<h1 id="human-control-audio">Human control audio</h1>

<p>It‚Äôs pretty fast to just say things, so collecting a bit of real data for validating things at the end is very doable. <em>If only there was a nice tool to handle keep track of all the wav files you need to generate</em>. Gemini code can easily cook one up in 20 minutes (that‚Äôs including a game of Minesweeper intermediate I played in between - really it was closer to 2 minutes).</p>

<p>Wielding this new tool, I can quickly generate 50 test set queries, and 50 validation set queries, which I‚Äôll evaluate on the end to end system later.</p>

<p><img src="/assets/images/data-collection-tool.png" alt="Let's generate some human data too" /></p>

<p><em>Let‚Äôs generate some human data too</em></p>

<p>With some transcriptions in hand, we can now start to train the embedding model.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Let‚Äôs take a look at the synthetic data for the spoken language entity linking system. We‚Äôll need a list of target entities, audio of users requesting those entities, and transcriptions of that audio.]]></summary></entry><entry><title type="html">Code-Switched Spoken Language Entity Linking: Intro</title><link href="http://localhost:4000/2025/07/03/spoken-language-entity-linking.html" rel="alternate" type="text/html" title="Code-Switched Spoken Language Entity Linking: Intro" /><published>2025-07-03T23:40:42+02:00</published><updated>2025-07-03T23:40:42+02:00</updated><id>http://localhost:4000/2025/07/03/spoken-language-entity-linking</id><content type="html" xml:base="http://localhost:4000/2025/07/03/spoken-language-entity-linking.html"><![CDATA[<p>Let‚Äôs say you‚Äôre on holiday with your voice assistant, and you want to get to <em>Bolzeschachtstra√üe</em>. You fire off a wakeup word, recording starts, you say: ‚Äú<em>Navigate to Bolzeschachtstra√üe</em>‚Äù. Your voice assistant hears ‚Äú<em>Baltzsakstasa</em>‚Äù. Another day, another place, this time <em>Zum Hallerbach</em>. Your voice assistant hears ‚Äú<em>Tsamhalaba</em>‚Äù. No results.</p>

<p><img src="/assets/images/typical_scenario.png" alt="Can we fix this?" width="40%" /></p>

<p><em>Can we fix this?</em></p>

<p>What is to blame? Clearly, speech recognition has a problem. It‚Äôs great if you can fix it. On the other hand, a sufficiently smart voice assistant really <em>should</em> be able to guess, <em>even from a severely distorted transcription</em>, what is really intended, as long as the distortion is not totally wild.</p>

<p>The following blogposts will demonstrate a simple system that can take such garbled transcriptions from speech recognition and offer more accurate alternatives.</p>

<p>‚ÄúCode-switched‚Äù means that the user will be speaking English, but will be requesting <em>non-English</em> entities. To keep things compact, I will define the universe of entities as being: German, Dutch and Swedish street names. A large enough universe to make the task challenging. I may add more as time goes on. ‚ÄúSpoken language‚Äù means that the input to the system is speech audio, to be transcribed by an ASR module.</p>

<p>The core of this system will be a text-based phonetic embedding model. The similarity function the embedding model should try to capture is this: ‚Äústring <em>x</em> should be close to string <em>y</em> if <em>x</em> is a likely transcription from an ASR system of someone speaking <em>y</em>‚Äù. So, phonetics is the main invariance that should be captured, but text normalization is another, as well as the output quirks of specific ASR models. The transformer backbone of the model will by a <strong>byt5</strong> encoder. I‚Äôll also consider a (not so?) silly baseline model based on <em>character ngrams</em>.</p>

<p>When people think of phonetic similarity, they may think of things like levenshtein distance. Unfortunately this won‚Äôt scale into a retrieval system, so we‚Äôll have to use a sparse or dense search index. The character ngram approach might be considered an approximation of edit distance. Out of the box, it already seems pretty weak because it doesn‚Äôt respect any of the invariances we are interested in. Maybe we can overcome this. In any case, there is, to my knowledge, no plug-and-play phonetic embedding model out there, so I‚Äôll have to build this out.</p>

<p>I‚Äôll take a look at a few different ASR models, phonetic embedding training strategies, and hybrid search &amp; reranking approaches. I‚Äôll use <em>Qdrant</em> as the vector search engine. For ASR, I will first consider <strong>openai/whisper-large-v3-turbo</strong> and <strong>nvidia/parakeet-tdt-0.6b-v2</strong>. But maybe some more recent multimodal LLMs which can accept audio input, like <strong>google/gemma-3n-E2B-it</strong> and <strong>microsoft/phi-4-instruct</strong>.</p>

<p><img src="/assets/images/embedding-training-pipeline.drawio.png" alt="Here's how it will all look" /></p>

<p><em>Here‚Äôs how the embedding model will be trained</em></p>

<p>The training data for the embedding model will come from <strong>hexgrad/Kokoro-82M</strong> text-to-speech. It has the advantage that you can synthesize from phonemes. The idea is to phonemize the street name part of the TTS queries using a locale-appropriate phonemizer, to approximate an English speaker who can roughly pronounce the non-English street name. If one of the good commercial providers can do code switching beter than this, I might spend O(‚Ç¨10) on that too.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Let‚Äôs say you‚Äôre on holiday with your voice assistant, and you want to get to Bolzeschachtstra√üe. You fire off a wakeup word, recording starts, you say: ‚ÄúNavigate to Bolzeschachtstra√üe‚Äù. Your voice assistant hears ‚ÄúBaltzsakstasa‚Äù. Another day, another place, this time Zum Hallerbach. Your voice assistant hears ‚ÄúTsamhalaba‚Äù. No results.]]></summary></entry></feed>