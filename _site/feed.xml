<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.10.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-07-04T16:18:22+02:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Token-mediated</title><subtitle>Work in progress üöß</subtitle><entry><title type="html">Code-Switched Spoken Language Entity Linking: Intro</title><link href="http://localhost:4000/2025/07/03/spoken-language-entity-linking.html" rel="alternate" type="text/html" title="Code-Switched Spoken Language Entity Linking: Intro" /><published>2025-07-03T23:40:42+02:00</published><updated>2025-07-03T23:40:42+02:00</updated><id>http://localhost:4000/2025/07/03/spoken-language-entity-linking</id><content type="html" xml:base="http://localhost:4000/2025/07/03/spoken-language-entity-linking.html"><![CDATA[<p>Let‚Äôs say you‚Äôre on holiday with your voice assistant, and you want to get to <em>Bolzeschachtstra√üe</em>. You fire off a wakeup word, recording starts, you say: ‚Äú<em>Navigate to Bolzeschachtstra√üe</em>‚Äù. Your voice assistant hears ‚Äú<em>Baltzsakstasa</em>‚Äù. Another day, another place, this time <em>Zum Hallerbach</em>. Your voice assistant hears ‚Äú<em>Tsamhalaba</em>‚Äù. No results.</p>

<p><img src="/assets/images/typical_scenario.png" alt="Can we fix this?" width="40%" /></p>

<p><em>Can we fix this?</em></p>

<p>What is to blame? Clearly, speech recognition has a problem. It‚Äôs great if you can fix it. On the other hand, a sufficiently smart voice assistant really <em>should</em> be able to guess, <em>even from an severely distorted transcription</em>, what is really intended, as long as the distortion is not totally wild.</p>

<p>The following blogposts will demonstrate a simple system that can take such garbled transcriptions from speech recognition and offer more accurate alternatives.</p>

<p>‚ÄúCode-switched‚Äù means that the user will be speaking English, but will be requesting <em>non-English</em> entities. To keep things compact, I will define the universe of entities as being: German, Dutch and Swedish street names. A large enough universe to make the task challenging. I may add more as time goes on. ‚ÄúSpoken language‚Äù means that the input to the system is speech audio, to be transcribed by an ASR module.</p>

<p>The core of this system will be a text-based phonetic embedding model. The similarity function the embedding model should try to capture is this: ‚Äústring <em>x</em> should be close to string <em>y</em> if <em>x</em> is a likely transcription from an ASR system of someone speaking <em>y</em>‚Äù. So, phonetics is the main invariance that should be captured, but text normalization is another, as well as the output quirks of specific ASR models. The transformer backbone of the model will by a <strong>byt5</strong> encoder. I‚Äôll also consider a (not so?) silly baseline model based on <em>character ngrams</em>.</p>

<p>When people think of phonetic similarity, they may think of things like levenshtein distance. Unfortunately this won‚Äôt scale into a retrieval system, so we‚Äôll have to use a sparse or dense search index. The character ngram approach might be considered an approximation of edit distance. Out of the box, it already seems pretty weak because it doesn‚Äôt respect any of the invariances we are interested in. Maybe we can overcome this. In any case, there is, to my knowledge, no plug-and-play phonetic embedding model out there, so I‚Äôll have to build this out.</p>

<p>I‚Äôll take a look at a few different ASR models, phonetic embedding training strategies, and hybrid search &amp; reranking approaches. I‚Äôll use <em>Qdrant</em> as the vector search engine. For ASR, I will first consider <strong>openai/whisper-large-v3-turbo</strong> and <strong>nvidia/parakeet-tdt-0.6b-v2</strong>. But maybe some more recent multimodal LLMs which can accept audio input, like <strong>google/gemma-3n-E2B-it</strong> and <strong>microsoft/phi-4-instruct</strong>.</p>

<p><img src="/assets/images/embedding-training-pipeline.drawio.png" alt="Here's how it will all look" /></p>

<p><em>Here‚Äôs how the embedding model will be trained</em></p>

<p>The training data for the embedding model will come from <strong>hexgrad/Kokoro-82M</strong> text-to-speech. It has the advantage that you can synthesize from phonemes. The idea is to phonemize the street name part of the TTS queries using a locale-appropriate phonemizer, to approximate an English speaker who can roughly pronounce the non-English street name. If one of the good commercial providers can do code switching beter than this, I might spend O(‚Ç¨10) on that too.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Let‚Äôs say you‚Äôre on holiday with your voice assistant, and you want to get to Bolzeschachtstra√üe. You fire off a wakeup word, recording starts, you say: ‚ÄúNavigate to Bolzeschachtstra√üe‚Äù. Your voice assistant hears ‚ÄúBaltzsakstasa‚Äù. Another day, another place, this time Zum Hallerbach. Your voice assistant hears ‚ÄúTsamhalaba‚Äù. No results.]]></summary></entry></feed>