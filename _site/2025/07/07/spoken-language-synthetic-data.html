<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Spoken Language Entity Linking: Synthetic Data | Token-mediated</title>
<meta name="generator" content="Jekyll v3.10.0" />
<meta property="og:title" content="Spoken Language Entity Linking: Synthetic Data" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Let‚Äôs take a look at the synthetic data for the spoken language entity linking system. We‚Äôll need a list of target entities, audio of users requesting those entities, and transcriptions of that audio." />
<meta property="og:description" content="Let‚Äôs take a look at the synthetic data for the spoken language entity linking system. We‚Äôll need a list of target entities, audio of users requesting those entities, and transcriptions of that audio." />
<link rel="canonical" href="http://localhost:4000/2025/07/07/spoken-language-synthetic-data.html" />
<meta property="og:url" content="http://localhost:4000/2025/07/07/spoken-language-synthetic-data.html" />
<meta property="og:site_name" content="Token-mediated" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-07-07T13:01:09+02:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Spoken Language Entity Linking: Synthetic Data" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-07-07T13:01:09+02:00","datePublished":"2025-07-07T13:01:09+02:00","description":"Let‚Äôs take a look at the synthetic data for the spoken language entity linking system. We‚Äôll need a list of target entities, audio of users requesting those entities, and transcriptions of that audio.","headline":"Spoken Language Entity Linking: Synthetic Data","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2025/07/07/spoken-language-synthetic-data.html"},"url":"http://localhost:4000/2025/07/07/spoken-language-synthetic-data.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Token-mediated" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Token-mediated</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Spoken Language Entity Linking: Synthetic Data</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2025-07-07T13:01:09+02:00" itemprop="datePublished">Jul 7, 2025
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>Let‚Äôs take a look at the synthetic data for the spoken language entity linking system. We‚Äôll need a list of target entities, audio of users requesting those entities, and transcriptions of that audio.</p>

<p>We can download all open street map data for our countries of interest via (Geofabrik)[geofabrik.de]. The data is a rather large protobuf file which can be converted by osmconvert into larger xml file. Attempts to efficiently parse this thing and extract street names via with a streaming xml parser stumped Gemini code - presumably it only has access to the same 15 year old blog posts as I could find. In the end, one can just grep for the relevant parts of the xml structure, reducing the file size considerably, then do the streaming xml parsing to precisely extract the information.</p>

<p>We end up with about 500k unique street names, which we can split into train, validation and test sets.</p>

<h1 id="text-to-speech">Text to speech</h1>

<p>The diagram in the <a href="https://nrgow.github.io/2025/07/03/spoken-language-entity-linking.html">first post</a> described the flow. <a href="https://huggingface.co/hexgrad/Kokoro-82M">Kokoro</a> can generate from phonemes, so we provide the phonemes for German street names from a German phonemizer, Dutch streets from a Dutch phonemizer, and so on - all espeak-ng phoneme models. There‚Äôs plenty of other things that could be done, like audio augmentations, considering alternative TTS providers. But there is enough to work with here for the moment. I sample 50k street names to generate audio for.</p>

<audio controls="">
  <source src="/assets/audio/plommonvagen_sv.wav" type="audio/mpeg" />
  Your browser does not support the audio element.
</audio>

<p><em>‚ÄúNavigate to Plommonv√§gen‚Äù</em></p>

<audio controls="">
  <source src="/assets/audio/wustenwetzdorf_de.wav" type="audio/mpeg" />
  Your browser does not support the audio element.
</audio>

<p><em>‚ÄúNavigate to W√ºstenwetzdorf‚Äù</em></p>

<audio controls="">
  <source src="/assets/audio/meerswal_nl.wav" type="audio/mpeg" />
  Your browser does not support the audio element.
</audio>

<p><em>‚ÄúNavigate to Meerswal‚Äù</em></p>

<h1 id="speech-to-text">Speech to text</h1>

<p>For transcribing our synthetic data, let‚Äôs start with <em>whisper-turbo</em> - this is a more compact version of a multilingual ASR model, which in my experience can sometimes do surprisingly well with code-switched queries.</p>

<p>As an alternative, we can look at a more recent model: Parakeet (<em>nvidia/parakeet-tdt-0.6b-v2</em>) is and performant model from nvidia. It has the benefit of being ~20x faster than whisper-turbo. It currently second on the <a href="https://huggingface.co/spaces/hf-audio/open_asr_leaderboard">open ASR leaderboard</a>. It‚Äôs English-only. Despite this project being about code-switching, one of the secret premises of this project is that we might even not need to know much about other languages, at least to do this somewhat artificial entity linking task. That‚Äôs if all goes well.</p>

<p>The character error rate over the entities is pretty similar for parakeet vs. whisper-turbo (41 vs 42 percent).</p>

<h2 id="multimodal-llms">Multimodal LLMs</h2>

<p>Some new multimodal LLMs have been released recently, most notably phi-4-multimodal and gemma-3n. These accept speech audio or text (images as well) as input. Why might such a model be interesting for a simple transcription task? Incorporation of a text-based prompt potentially allows for zero-shot or few-shot customization of the ASR component without requiring finetuning. Let‚Äôs see if that works with Gemma 3n <em>google/gemma-3n-E2B-it</em>.</p>

<h3 id="prompt-help-1">Prompt help 1</h3>

<p>All queries have the form ‚Äúnavigate to <street>", which I use to then extract the street name via regex. I already have to Gemma give a little help via the system prompt.</street></p>

<blockquote>
  <p>‚ÄúYou are a helpful assistant who can transcribe audio. Users typically issue commands <em>in the imperative voice</em>.‚Äù</p>
</blockquote>

<p>The results are pretty bad. Character error rate is 57%. The transcriptions are not always phonetically plausible, rather making phonetically poorly-grounded semantic jumps to other entiti es, but we‚Äôre not here to judge ASR models. We‚Äôre trying to train a model that can correct for ASR errors, so the transcription may be valuable for that.</p>

<h3 id="prompt-help-2">Prompt help 2</h3>

<p>As an experiment, let‚Äôs give Gemma some more help with an enhanced task prompt:</p>

<blockquote>
  <p>Transcribe this audio file which has this structure: ‚ÄòNavigate to &lt;german/dutch/swedish street name&gt;‚Äô</p>
</blockquote>

<p>Things start to look better. Character error rate decreases to 46%: worse than parakeet or whisper but closing the gap. In less than one percent of cases, it takes the prompt a little bit too literally, and outputs ‚Äú<em>navigate to &lt;german/dutch/swedish street name&gt;</em>‚Äù.</p>

<p>This example may be a little contrived, as one might rarely be able to give such a big hint, but it nicely demonstrates the flexibility and responsiveness of a multimodal model to enhanced context. In a real system, such context engineering could go in the direction of personalization and memory, or guidelines for particular terminology.</p>

<h2 id="interim-character-error-rate-evaluation">Interim character error-rate evaluation</h2>

<p>We can measure the ASR on our synthetic data, taking the character error rate of just the entity name. This evaluation is just for demonstration - maybe it is roughly correlated with end to end task performance.</p>

<table>
  <thead>
    <tr>
      <th>¬†</th>
      <th>parakeet</th>
      <th>whisper-turbo</th>
      <th>gemma_simpleprompt</th>
      <th>gemma_hintprompt</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>cer</td>
      <td>0.43</td>
      <td>0.43</td>
      <td>0.57</td>
      <td>0.46</td>
    </tr>
  </tbody>
</table>

<h2 id="transcription-examples">Transcription examples</h2>

<table>
  <thead>
    <tr>
      <th>entity</th>
      <th>parakeet</th>
      <th>whisper-turbo</th>
      <th>gemma_simpleprompt</th>
      <th>gemma_hintprompt</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>K√∂lv√§gen</td>
      <td>Schellwegan</td>
      <td>Shellvegan</td>
      <td>Shelvegen</td>
      <td>Shelvegan</td>
    </tr>
    <tr>
      <td>Pfarrer-Held-Stra√üe</td>
      <td>Pfaterhilshthasa</td>
      <td>Pfaderhildst√§dt Hasse</td>
      <td>Powerchords to Hassa</td>
      <td>Pfahlstr</td>
    </tr>
    <tr>
      <td>Park de Wervelaan</td>
      <td>Parc de Vervelin</td>
      <td>Parc de Vervalan</td>
      <td>Park de Verelan</td>
      <td>Park de Verellen</td>
    </tr>
    <tr>
      <td>Untere Pfeiferm√ºhle</td>
      <td>Unterup Phi formula</td>
      <td>Unterupfi Formula</td>
      <td>Unter a five formula</td>
      <td>Unter der Pfeiferstra√üe</td>
    </tr>
    <tr>
      <td>Posses v√§g</td>
      <td>Pausus Veg</td>
      <td>Posis Veg</td>
      <td>Passus Veg</td>
      <td>Passusweg</td>
    </tr>
    <tr>
      <td>Am Springgarten</td>
      <td>Mspttingaten</td>
      <td>Amschptingaten</td>
      <td>Amsterdam</td>
      <td>amshiptingaten</td>
    </tr>
    <tr>
      <td>Bro√§ngsv√§gen</td>
      <td>Brunswagen</td>
      <td>Bruinswagen</td>
      <td>Braunschweig</td>
      <td>Braunschweig</td>
    </tr>
    <tr>
      <td>Alander Weg</td>
      <td>Aulender Vake</td>
      <td>All under VAKE</td>
      <td>all under wake</td>
      <td>Allenderwiek</td>
    </tr>
    <tr>
      <td>Im Alengarten</td>
      <td>him all in Gatson</td>
      <td>Im Allenghattan</td>
      <td>M All in Gaten</td>
      <td>am Allingaten</td>
    </tr>
    <tr>
      <td>Nordbrink</td>
      <td>NodPink</td>
      <td>Nodbdink</td>
      <td>Notepad</td>
      <td>Nordpink</td>
    </tr>
    <tr>
      <td>Siegelhof</td>
      <td>Ziegelhoof</td>
      <td>Ziegelhof</td>
      <td>Ziegelhof</td>
      <td>Zeughof</td>
    </tr>
    <tr>
      <td>Im Eisenfeld</td>
      <td>him Eisenfelt</td>
      <td>M. Eisenfeld</td>
      <td>Imfeld</td>
      <td>Immenfeld</td>
    </tr>
  </tbody>
</table>

<h1 id="human-control-audio">Human control audio</h1>

<p>It‚Äôs pretty fast to just say things, so collecting a bit of real data for validating things at the end is very doable. <em>If only there was a nice tool to handle keep track of all the wav files you need to generate</em>. Gemini code can easily cook one up in 20 minutes (that‚Äôs including a game of Minesweeper intermediate I played in between - really it was closer to 2 minutes).</p>

<p>Wielding this new tool, I can quickly generate 50 test set queries, and 50 validation set queries, which I‚Äôll evaluate on the end to end system later.</p>

<p><img src="/assets/images/data-collection-tool.png" alt="Let's generate some human data too" /></p>

<p><em>Let‚Äôs generate some human data too</em></p>

<p>With some transcriptions in hand, we can now start to train the embedding model.</p>

  </div><a class="u-url" href="/2025/07/07/spoken-language-synthetic-data.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Token-mediated</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Token-mediated</li><li><a class="u-email" href="mailto:nrgow@protonmail.com">nrgow@protonmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/nrgow"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">nrgow</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Work in progress üöß</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
